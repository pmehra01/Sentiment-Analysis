{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e00ac6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "C:\\Users\\Asus\\AppData\\Local\\Temp\\ipykernel_8400\\1985160743.py:17: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  cleaned_data = y.str.replace(r'^\\d+\\.\\s', '')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: vaderSentiment in c:\\users\\asus\\anaconda3\\lib\\site-packages (3.3.2)\n",
      "Requirement already satisfied: requests in c:\\users\\asus\\anaconda3\\lib\\site-packages (from vaderSentiment) (2.28.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests->vaderSentiment) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests->vaderSentiment) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests->vaderSentiment) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests->vaderSentiment) (2022.9.14)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "df = pd.read_csv(\"CAA.csv\")\n",
    "y =df[\"text\"]\n",
    "size = df.shape\n",
    "# Remove leading numbers followed by a period and space using regular expressions\n",
    "cleaned_data = y.str.replace(r'^\\d+\\.\\s', '')\n",
    "\n",
    "# Convert the Series to a list\n",
    "cleaned_data = cleaned_data.tolist()\n",
    "cleaned_data = [entry for entry in cleaned_data if isinstance(entry, str)]\n",
    "# Convert the list to a set to remove duplicates\n",
    "unique_data = set(cleaned_data)\n",
    "\n",
    "# Convert the set back to a list if needed\n",
    "cleaned_data = list(unique_data)\n",
    "\n",
    "# Print the unique tweets\n",
    "# print(unique_data_list[:20])\n",
    "# specific_words = [\"farmer\", \"#farmerprotest\", \"farmer protest\"]\n",
    " # Specify the words you want to check for\n",
    "specific_words = [\"caa\",\"#caa\"]\n",
    "\n",
    "# Filter out entries that do not contain any of the specific words (case-insensitive)\n",
    "filtered_data = [entry for entry in cleaned_data if any(word.lower() in entry.lower() for word in specific_words)]\n",
    "\n",
    "# Print the filtered data\n",
    "# for entry in filtered_data:\n",
    "#     print(entry)\n",
    "# type(filtered_data)\n",
    "import re\n",
    "\n",
    "# Define a regular expression pattern to match links or words containing \"www.\"\n",
    "pattern = r'\\b(?:https?://|www\\.)\\S+\\b'\n",
    "\n",
    "# Remove links or words containing \"www.\" from the data\n",
    "new_cleaned_data = [re.sub(pattern, '', entry) for entry in filtered_data]\n",
    "\n",
    "# Print the cleaned data\n",
    "# for entry in new_cleaned_data:\n",
    "#     print(entry)\n",
    "def preprocess_text(text_list):\n",
    "    preprocessed_texts = []\n",
    "    for text in text_list:\n",
    "        # Tokenization\n",
    "        tokens = word_tokenize(text)\n",
    "        # Convert to lowercase\n",
    "        tokens = [token.lower() for token in tokens]\n",
    "        # Remove punctuation\n",
    "        tokens = [token for token in tokens if token not in string.punctuation]\n",
    "        # Remove stop words\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        tokens = [token for token in tokens if token not in stop_words]\n",
    "        # Lemmatization\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "        preprocessed_texts.append(tokens)\n",
    "    return preprocessed_texts\n",
    "\n",
    "preprocessed_text_list = [' '.join(tokens) for tokens in preprocess_text(cleaned_data)]\n",
    "# print(preprocessed_text_list)\n",
    "def clean_text(text):\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "\n",
    "    # Remove non-alphanumeric characters and symbols except whitespace\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    return text\n",
    "statements = preprocessed_text_list\n",
    "cleaned_statements = [clean_text(statement) for statement in statements]\n",
    "# nlp approach using textblob\n",
    "from textblob import TextBlob\n",
    "\n",
    "def label_sentiment(text):\n",
    "    analysis = TextBlob(text)\n",
    "    if analysis.sentiment.polarity > 0:\n",
    "        return 1  # Positive sentiment\n",
    "    elif analysis.sentiment.polarity == 0:\n",
    "        return 0  # Neutral sentiment\n",
    "    else:\n",
    "        return -1  # Negative sentiment\n",
    "\n",
    "# Example list of strings\n",
    "# list_of_statements =cleaned_statements\n",
    "\n",
    "# Label each statement\n",
    "textblob_labels = [label_sentiment(statement) for statement in cleaned_statements]\n",
    "!pip install vaderSentiment\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "# Create an instance of the SentimentIntensityAnalyzer class\n",
    "\n",
    "text = cleaned_statements\n",
    "# List to store sentiment labels\n",
    "actual_labels_list_vader = []\n",
    "\n",
    "# Analyze sentiment for each text\n",
    "for text in text:\n",
    "    # Analyze sentiment of the text\n",
    "    scores = analyzer.polarity_scores(text)\n",
    "\n",
    "    # Convert sentiment score to label\n",
    "    if scores['compound'] >= 0.05:\n",
    "        actual_labels_list_vader.append(1)  # Positive sentiment\n",
    "    elif scores['compound'] <= -0.05:\n",
    "        actual_labels_list_vader.append(-1) # Negative sentiment\n",
    "    else:\n",
    "        actual_labels_list_vader.append(0)   # Neutral sentiment\n",
    "\n",
    "def analyze_sentiment_vader(text):\n",
    "  \"\"\"\n",
    "  Analyzes the sentiment of a statement using VADER (Valence Aware Dictionary and sEntiment Reasoner).\n",
    "\n",
    "  Args:\n",
    "      text: The text statement for sentiment analysis.\n",
    "\n",
    "  Returns:\n",
    "      A list containing:\n",
    "          - Sentiment score (float): A compound score between -1 (negative) and 1 (positive).\n",
    "          - Sentiment label (int): 1 for positive, -1 for negative, 0 for neutral.\n",
    "  \"\"\"\n",
    "\n",
    "  analyzer = SentimentIntensityAnalyzer()\n",
    "  scores = analyzer.polarity_scores(text)\n",
    "\n",
    "  sentiment_label = 0\n",
    "  if scores['compound'] >= 0.05:\n",
    "    sentiment_label = 1  # Positive sentiment\n",
    "  elif scores['compound'] <= -0.05:\n",
    "    sentiment_label = -1  # Negative sentiment\n",
    "\n",
    "  return sentiment_label\n",
    "\n",
    "vedar_labels = actual_labels_list_vader\n",
    "textblob_labels_final = textblob_labels[2000:]\n",
    "vedar_labels_final = vedar_labels[2000:]\n",
    "# len(vedar_labels)\n",
    "# len(textblob_labels)\n",
    "# 2000 elemets label stored\n",
    "data_dict = {}\n",
    "\n",
    "for statement, label in zip(cleaned_statements[:1999], textblob_labels[:1999]):\n",
    "    data_dict[statement] = label\n",
    "\n",
    "# Printing the dictionary\n",
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "for statement, label in zip(cleaned_statements[:1999], textblob_labels[:1999]):\n",
    "    X_train.append(statement)\n",
    "    y_train.append(label)\n",
    "    \n",
    "from sklearn import svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "\n",
    "svm_model = svm.SVC(kernel='linear')\n",
    "svm_model.fit(X_train_vectorized, y_train)\n",
    "def analyze_sentiment_svm(text):\n",
    "  \"\"\"\n",
    "  Analyzes the sentiment of a statement using the globally defined SVM model and TF-IDF vectorizer.\n",
    "\n",
    "  Args:\n",
    "      text: The text statement for sentiment analysis.\n",
    "\n",
    "  Returns:\n",
    "      The predicted sentiment label (e.g., 1 for positive, -1 for negative).\n",
    "  \"\"\"\n",
    "\n",
    "  # Vectorize the text\n",
    "  text_vectorized = vectorizer.transform([text])\n",
    "\n",
    "  # Predict sentiment using the SVM model\n",
    "  sentiment_label = svm_model.predict(text_vectorized)[0]\n",
    "\n",
    "  return sentiment_label\n",
    "\n",
    "unlabeled_data = cleaned_statements[2000:]\n",
    "X_unlabeled_vectorized = vectorizer.transform(unlabeled_data)\n",
    "# 3. Make predictions\n",
    "predicted_labels = svm_model.predict(X_unlabeled_vectorized)\n",
    "\n",
    "# Store the predicted labels in a list\n",
    "predicted_labels_list = predicted_labels.tolist()\n",
    "svm_labels_final = predicted_labels_list\n",
    "\n",
    "# finding mode\n",
    "from statistics import mode\n",
    "from collections import Counter\n",
    "\n",
    "# Function to find the mode of three elements\n",
    "def find_mode(a, b, c):\n",
    "    count = Counter([a, b, c])\n",
    "    most_common = count.most_common(1)\n",
    "    return most_common[0][0]\n",
    "\n",
    "# List to store the mode of corresponding elements\n",
    "final_labels = []\n",
    "\n",
    "# Iterate through the lists and find the mode of each set of corresponding elements\n",
    "for svm, vedar, textblob in zip(svm_labels_final, vedar_labels_final, textblob_labels_final):\n",
    "    final_labels.append(find_mode(svm, vedar, textblob))\n",
    "\n",
    "# Print the final labels\n",
    "# print(final_labels)\n",
    "\n",
    "# previous method\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Count the occurrences of each label\n",
    "label_counts = {-1: 0, 0: 0, 1: 0}\n",
    "for label in final_labels:\n",
    "    label_counts[label] += 1\n",
    "\n",
    "# Plot the distribution\n",
    "plt.bar(label_counts.keys(), label_counts.values(), color=['red', 'blue', 'green'])\n",
    "plt.xlabel('Sentiment Label')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Sentiment Labels')\n",
    "plt.xticks([-1, 0, 1], ['-1 (Negative)', '0 (Neutral)', '1 (Positive)'])\n",
    "plt.show()\n",
    "\n",
    "# Calculate percentages\n",
    "total_statements = len(cleaned_statements)\n",
    "percentage_label_counts = {label: (count / total_statements) * 100 for label, count in label_counts.items()}\n",
    "\n",
    "# Plot the distribution\n",
    "plt.bar(percentage_label_counts.keys(), percentage_label_counts.values(), color=['red', 'blue', 'green'])\n",
    "plt.xlabel('Sentiment Label')\n",
    "plt.ylabel('Percentage')\n",
    "plt.title('Distribution of Sentiment Labels')\n",
    "plt.xticks([-1, 0, 1], ['-1 (Negative)', '0 (Neutral)', '1 (Positive)'])\n",
    "plt.show()\n",
    "\n",
    "# Pie chart\n",
    "plt.pie(percentage_label_counts.values(), labels=percentage_label_counts.keys(), autopct='%1.1f%%', colors=['red', 'blue', 'green'])\n",
    "plt.title('Distribution of Sentiment Labels')\n",
    "plt.show()\n",
    "\n",
    "def analyze_sentiment_ensemble(text):\n",
    "  \"\"\"\n",
    "  Analyzes sentiment of a statement using an ensemble of three models and returns the mode.\n",
    "\n",
    "  Args:\n",
    "      text: The text statement for sentiment analysis.\n",
    "\n",
    "  Returns:\n",
    "      The mode of sentiment labels (-1 for negative, 0 for neutral, 1 for positive)\n",
    "          based on the results from the three models.\n",
    "  \"\"\"\n",
    "\n",
    "  # Call sentiment analysis functions\n",
    "  vader_result = analyze_sentiment_vader(text)  # Assuming it returns a sentiment label\n",
    "  label_sentiment_result = label_sentiment(text)  # Assuming it returns a sentiment label\n",
    "  svm_result = analyze_sentiment_svm(text)  # Assuming it returns a sentiment label\n",
    "\n",
    "  # Store results\n",
    "  results = [vader_result, label_sentiment_result, svm_result]\n",
    "\n",
    "  # Count occurrences of each sentiment label\n",
    "  from collections import Counter\n",
    "  sentiment_counts = Counter(results)\n",
    "\n",
    "  # Find the mode (most frequent sentiment)\n",
    "  mode_sentiment = sentiment_counts.most_common(1)[0][0]\n",
    "\n",
    "  return mode_sentiment\n",
    "\n",
    "\n",
    "\n",
    "import speech_recognition as sr\n",
    "import pyttsx3\n",
    "\n",
    "# Initialize speech recognition and text-to-speech engines\n",
    "recognizer = sr.Recognizer()\n",
    "engine = pyttsx3.init()\n",
    "\n",
    "def speak(text):\n",
    "    \"\"\"Speaks the given text using the text-to-speech engine.\"\"\"\n",
    "    engine.say(text)\n",
    "    engine.runAndWait()\n",
    "\n",
    "def listen():\n",
    "    \"\"\"Listens for user input using speech recognition.\"\"\"\n",
    "    with sr.Microphone() as source:\n",
    "        recognizer.adjust_for_ambient_noise(source)  # Adjust for ambient noise\n",
    "        print(\"Listening...\")\n",
    "        audio = recognizer.listen(source, timeout=10, phrase_time_limit=10)  # Listen for longer period\n",
    "\n",
    "    try:\n",
    "        text = recognizer.recognize_google(audio)\n",
    "        print(\"You said:\", text)\n",
    "        return text.lower()  # Convert to lowercase for case-insensitive analysis\n",
    "    except sr.UnknownValueError:\n",
    "        print(\"Sorry, I could not understand the audio.\")\n",
    "        return None\n",
    "    except sr.RequestError as e:\n",
    "        print(f\"Could not request results from Google Speech Recognition service; {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main loop for voice assistant interaction.\"\"\"\n",
    "    speak(\"Hello! I am Senti, your voice assistant. How can I help you today?\")\n",
    "    while True:\n",
    "        text = listen()\n",
    "        if text:\n",
    "            if \"stop\" in text or \"exit\" in text:\n",
    "                speak(\"Goodbye!\")\n",
    "                break\n",
    "            sentiment = analyze_sentiment_ensemble(text)\n",
    "            if sentiment == -1:\n",
    "                speak(\"That sounds negative. Is there anything I can help you with?\")\n",
    "            elif sentiment == 0:\n",
    "                speak(\"Seems like a neutral statement. Do you have anything else you'd like to say?\")\n",
    "            else:\n",
    "                speak(f\"That sounds positive! What can I do for you today?\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874036c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Please enter some text:\")\n",
    "text = input()\n",
    "print(analyze_sentiment_ensemble(text))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
